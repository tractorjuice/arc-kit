# JSP 936 AI Assurance Guide

A comprehensive guide to MOD AI assurance documentation using JSP 936 methodology with ArcKit.

---

## What is JSP 936?

**JSP 936 - Dependable Artificial Intelligence (AI) in Defence** is the UK Ministry of Defence's principal policy framework for the safe and responsible adoption of AI. Published November 2024, it ensures AI systems in defence projects meet rigorous ethical, safety, and security standards.

### Why JSP 936 Matters

Without JSP 936 AI assurance:
- ❌ Ethical risks unidentified until late
- ❌ No systematic AI safety assessment
- ❌ Bias and harm discovered in deployment
- ❌ Unclear accountability for AI decisions
- ❌ No approval pathway for defence AI
- ❌ Audit findings on AI governance gaps

With JSP 936 AI assurance:
- ✅ Proactive ethical risk assessment
- ✅ Systematic 5 principles evaluation
- ✅ Bias testing and mitigation before deployment
- ✅ Clear accountability (RAISOs, Ethics Managers)
- ✅ Defined approval pathways (2PUS → Defence → TLB)
- ✅ Continuous monitoring and re-assessment

---

## JSP 936 Framework

### Five Ethical Principles

1. **Human-Centricity** - AI respects human dignity, rights, and values. Humans remain in control.
2. **Responsibility** - Clear accountability throughout AI lifecycle. Meaningful human control maintained.
3. **Understanding** - AI systems are understandable, explainable, and appropriately transparent.
4. **Bias and Harm Mitigation** - AI designed to minimise bias and prevent harm.
5. **Reliability** - AI is robust, secure, and performs consistently within defined boundaries.

### Five Risk Classification Levels

| Score | Classification | Approval Pathway |
|-------|----------------|------------------|
| 20-25 | **Critical** | 2PUS or Ministers |
| 15-19 | **Severe** | Defence-Level (JROC/IAC) |
| 10-14 | **Major** | Defence-Level (JROC/IAC) |
| 5-9 | **Moderate** | TLB-Level (delegated) |
| 1-4 | **Minor** | TLB-Level (delegated) |

**Risk Score = Likelihood (1-5) × Impact (1-5)**

### Eight AI Lifecycle Phases

1. **Planning** - Use case justification, initial ethical screening
2. **Requirements** - FR/NFR/ETH/SAF/SEC requirements, HAZOP analysis
3. **Architecture** - System design, human-AI interaction, failure modes
4. **Algorithm Design** - Algorithm selection, explainability approach
5. **Model Development** - Training, model card, bias analysis
6. **Verification & Validation** - Testing, V&V report, UAT
7. **Integration & Use** - Deployment, operational procedures, monitoring
8. **Quality Assurance** - Compliance matrix, ethical review, security audit

### Governance Structure

- **RAISO** (Responsible AI Senior Officer) - Overall AI governance
- **Ethics Manager** - Day-to-day ethical oversight
- **Independent Ethics Assurance** - For Critical classification systems

---

## When to Run JSP 936 Assessment

Run `/arckit.jsp-936` when your project includes AI/ML components:

```
Phase 1: Discovery/Alpha
1. /arckit.stakeholders      ← Identify RAISO, Ethics Manager
2. /arckit.requirements      ← Initial functional requirements
3. /arckit.jsp-936           ← AI ASSURANCE ASSESSMENT (START HERE)
4. /arckit.architecture      ← Design human-AI interaction
5. /arckit.mod-secure        ← AI-specific security threats

Phase 2: Beta
6. /arckit.jsp-936           ← Update as AI system develops
7. /arckit.backlog           ← Track JSP 936 compliance tasks

Phase 3: Live
8. /arckit.jsp-936           ← Annual re-assessment
9. Continuous monitoring     ← Drift detection, retraining triggers
```

**CRITICAL**: Run JSP 936 assessment **early** (Discovery/Alpha) to identify ethical risks before significant development investment.

---

## Creating JSP 936 Documentation with ArcKit

### Step 1: Identify AI/ML Components

**Before running JSP 936**, ensure you have:
- Project requirements documented
- Architecture design (at least high-level)
- Clear understanding of AI/ML components

```bash
# First, document your requirements and architecture
/arckit.requirements Define requirements for [your project]
/arckit.architecture Design architecture for [your project]
```

### Step 2: Run JSP 936 Assessment

```bash
/arckit.jsp-936 Generate AI assurance documentation for [your project]
```

**Examples**:
```bash
# For existing ArcKit project
/arckit.jsp-936 Generate JSP 936 documentation for threat detection system

# With specific AI component
/arckit.jsp-936 Assess facial recognition system for base access control

# For multiple AI components
/arckit.jsp-936 Create AI assurance for autonomous drone navigation and threat analysis system
```

### Step 3: Review the Output

ArcKit creates `.arckit/jsp-936/jsp-936-assessment.md` containing:

1. **Executive Summary** - Risk classification, key findings, approval status
2. **AI System Inventory** - All AI/ML components catalogued
3. **Ethical Risk Assessment** - Likelihood × Impact matrix for each component
4. **Five Principles Compliance** - Comprehensive assessment per principle
5. **Lifecycle Documentation** - Evidence for all 8 phases
6. **Governance & Approval** - RAISO, Ethics Manager, approval pathway
7. **Human-AI Teaming** - Training, dashboard, trust calibration, overrides
8. **Secure by Design** - AI-specific threats and controls
9. **Supplier Assurance** - Third-party AI component assessment (if applicable)
10. **Continuous Monitoring** - Real-time monitoring, drift detection, retraining triggers
11. **Compliance Matrix** - All 27 JSP 936 requirements mapped to evidence
12. **Appendices** - Model cards, bias reports, V&V reports, security tests

---

## AI Component Types Identified

ArcKit automatically identifies 7 categories of AI/ML components:

### 1. Machine Learning Models
Supervised, unsupervised, reinforcement learning, deep learning
- **Example**: Predictive maintenance model for vehicle fleets

### 2. AI Algorithms
Decision trees, SVMs, Bayesian networks, expert systems
- **Example**: Rule-based threat classification system

### 3. Autonomous Systems
Autonomous vehicles/drones, robotic systems, automated decision-making
- **Example**: Autonomous reconnaissance drone with obstacle avoidance

### 4. Decision Support Systems
Recommendation engines, risk assessment tools, predictive analytics
- **Example**: Intelligence analysis tool that highlights potential threats

### 5. Natural Language Processing
Chatbots, text classification, NER, machine translation
- **Example**: Automated situation report analysis from radio communications

### 6. Computer Vision
Object detection, face recognition, image classification, video analysis
- **Example**: Satellite imagery analysis for infrastructure changes

### 7. Generative AI
Large language models, image generation, synthetic data generation
- **Example**: LLM for briefing document summarization

---

## Ethical Risk Assessment: Likelihood × Impact Matrix

### Impact Assessment (Scale: 1-5)

**Five Impact Dimensions**:
1. **Human Safety and Wellbeing** - Physical/psychological harm, human rights
2. **Operational Effectiveness** - Mission success, capability impact
3. **Legal and Ethical Compliance** - Laws of armed conflict, international law
4. **Public Trust and Reputation** - MOD reputation, public confidence
5. **International Obligations** - Treaty commitments, alliances

**Impact Levels**:
- **5 - Catastrophic**: Loss of life, mission failure, international incident
- **4 - Major**: Severe injuries, significant mission degradation, legal violations
- **3 - Moderate**: Minor injuries, noticeable performance impact, ethical concerns
- **2 - Minor**: Limited impact, manageable within existing processes
- **1 - Insignificant**: Minimal impact, easily recovered

### Likelihood Assessment (Scale: 1-5)

**Five Likelihood Factors**:
1. **Technology Maturity (TRL)** - Technology Readiness Level 1-9
2. **Data Quality and Availability** - Training data quality, bias, completeness
3. **Algorithm Complexity** - Model complexity, parameter count, interpretability
4. **Operational Environment** - Environmental variability, adversarial conditions
5. **Human Factors and Training** - Operator training, trust calibration, workload

**Likelihood Levels**:
- **5 - Almost Certain**: >80% probability, expected to occur
- **4 - Likely**: 50-80%, will probably occur
- **3 - Possible**: 30-50%, might occur at some time
- **2 - Unlikely**: 10-30%, could occur but not expected
- **1 - Rare**: <10%, only in exceptional circumstances

### Risk Classification Matrix

| Impact → | 1 | 2 | 3 | 4 | 5 |
|----------|---|---|---|---|---|
| **Likelihood ↓** |
| **5** | 5 (Mod) | 10 (Maj) | 15 (Sev) | 20 (Crit) | 25 (Crit) |
| **4** | 4 (Min) | 8 (Mod) | 12 (Maj) | 16 (Sev) | 20 (Crit) |
| **3** | 3 (Min) | 6 (Mod) | 9 (Mod) | 12 (Maj) | 15 (Sev) |
| **2** | 2 (Min) | 4 (Min) | 6 (Mod) | 8 (Mod) | 10 (Maj) |
| **1** | 1 (Min) | 2 (Min) | 3 (Min) | 4 (Min) | 5 (Mod) |

**Unacceptable Risk**: STOP immediately if significant harms are imminent, catastrophic risks present, or system behaving outside acceptable bounds.

---

## Five Ethical Principles Deep Dive

### Principle 1: Human-Centricity

**Key Requirements**:
- Affected stakeholders identified and consulted
- Human rights impact assessed
- Human-AI interaction model defined (in-loop/on-loop/out-of-loop)
- Override capability provided where appropriate
- Transparency to users about AI involvement

**ArcKit Assessment**:
- Stakeholder impact analysis
- Rights considerations (privacy, autonomy, dignity)
- Human control mechanisms
- User understanding of AI decisions

### Principle 2: Responsibility

**Key Requirements**:
- Clear accountability mapping (RACI)
- RAISO and Ethics Manager assigned
- Meaningful human control maintained
- Decision authority defined
- Decision traceability and audit trail

**ArcKit Assessment**:
- Accountability matrix
- Governance roles assigned
- Control level evaluation (Full/Substantial/Limited/None)
- Decision logging and explainability

### Principle 3: Understanding

**Key Requirements**:
- Explainability method implemented (LIME, SHAP, etc.)
- Model card documenting system details
- Training programme for operators and commanders
- Known limitations documented and communicated
- Technical and operational documentation complete

**ArcKit Assessment**:
- Explainability approach and target audience
- Documentation completeness check
- Training programme evaluation
- Limitation awareness verification

### Principle 4: Bias and Harm Mitigation

**Key Requirements**:
- Bias testing conducted across protected characteristics
- Bias sources identified (data, algorithmic, deployment, feedback loop)
- Potential harms catalogued with likelihood and severity
- Vulnerable groups identified
- Mitigation strategies implemented (data, algorithmic, operational)

**ArcKit Assessment**:
- Bias test methodology and results
- Harm identification for each AI component
- Mitigation effectiveness evaluation
- Ongoing bias monitoring plan

### Principle 5: Reliability

**Key Requirements**:
- Performance boundaries defined (minimum acceptable metrics)
- Robustness tested (adversarial, environmental, edge cases)
- AI-specific security threats assessed and controlled
- Verification & Validation report completed
- Out-of-distribution detection implemented

**ArcKit Assessment**:
- Performance metrics vs. requirements
- Robustness testing results
- Security control verification
- Reliability evidence (test coverage, failure rate)

---

## Human-AI Teaming Models

### Human-in-Loop (HIL)

**Definition**: Human reviews **every** AI decision before action is taken.

**When to Use**:
- Critical decisions (life/death, strategic impact)
- Low trust in AI system (early deployment, novel scenarios)
- Legal requirement for human approval

**Example**: Weapon targeting system where operator must approve every engagement.

**JSP 936 Requirements**:
- Clear approval workflow
- Time available for review
- Training on decision criteria
- Override always available

### Human-on-Loop (HOL)

**Definition**: Human monitors AI with ability to intervene when needed.

**When to Use**:
- Semi-critical decisions
- High trust in AI but edge cases possible
- Time-sensitive but not instantaneous

**Example**: Autonomous vehicle navigation where operator monitors and can take control.

**JSP 936 Requirements**:
- Real-time monitoring dashboard
- Clear intervention triggers
- Alert mechanisms for anomalies
- Training on when to intervene

### Human-out-of-Loop (HOOL)

**Definition**: AI operates autonomously within predefined constraints set by humans.

**When to Use**:
- Low-risk decisions
- Very time-sensitive operations
- Highly predictable environments

**Example**: Automated logistics optimization with human-defined constraints.

**JSP 936 Requirements**:
- Robust constraint validation
- Continuous monitoring for constraint violations
- Automatic shutdown if boundaries exceeded
- Regular human review of outputs

---

## AI-Specific Security Threats

ArcKit assesses 6 categories of AI-specific threats:

### 1. Adversarial Examples
**Threat**: Carefully crafted inputs that fool the AI.
**Example**: Slight image perturbation causes tank misclassified as truck.
**Mitigation**: Adversarial training, input validation, ensemble methods.

### 2. Data Poisoning
**Threat**: Malicious data injected into training set.
**Example**: Attacker adds biased data to training pipeline to degrade performance.
**Mitigation**: Data provenance, data validation, anomaly detection.

### 3. Model Extraction
**Threat**: Adversary steals model through repeated queries.
**Example**: External actor reconstructs threat detection model by querying API.
**Mitigation**: Query limits, differential privacy, model obfuscation.

### 4. Model Inversion
**Threat**: Adversary reconstructs training data from model.
**Example**: Attacker recovers sensitive personnel photos from face recognition system.
**Mitigation**: Differential privacy, aggregation, access controls.

### 5. Backdoor Attacks
**Threat**: Hidden triggers cause malicious behavior.
**Example**: Model behaves normally except when specific trigger appears, then fails.
**Mitigation**: Model inspection, trigger detection, diverse training data.

### 6. Concept Drift
**Threat**: Real-world data distribution changes over time, degrading performance.
**Example**: Threat patterns evolve, detection system becomes less accurate.
**Mitigation**: Continuous monitoring, drift detection, automated retraining.

---

## Continuous Monitoring and Re-Assessment

### Real-Time Monitoring

**Performance Metrics**:
- Accuracy, precision, recall, F1-score
- Latency, throughput, error rate
- Alert thresholds for degradation

**Bias Monitoring**:
- Fairness metrics by demographic group
- Continuous bias detection
- Alert on bias threshold breach

**Security Monitoring**:
- Adversarial input detection
- Anomaly detection
- Attack pattern recognition

**Drift Detection**:
- Data drift (input distribution changes)
- Concept drift (model performance degradation)
- Statistical tests and performance tracking

### Retraining Triggers

**Automatic Triggers**:
- Performance drops below threshold
- Significant data drift detected
- Bias increases beyond acceptable level
- New data volume threshold reached

**Manual Triggers**:
- Operational environment changes
- New requirements added
- Security vulnerabilities discovered
- Ethical concerns arise
- Scheduled retraining (e.g., quarterly)

### Annual JSP 936 Re-Assessment

**Required Activities**:
- Full re-assessment of all Five Principles
- Risk re-classification
- Lifecycle documentation review
- Governance structure review
- Security audit
- Bias assessment
- Stakeholder consultation
- Re-approval by original authority

### System Retirement Criteria

**When to Retire AI System**:
- Risk classification increases to unacceptable level
- Performance degrades below minimum acceptable
- Ethical concerns cannot be mitigated
- Security vulnerabilities cannot be remediated
- Technology becomes obsolete
- Cost-benefit no longer favorable

---

## Approval Pathways

### Critical (Score 20-25) → 2PUS or Ministerial

**Requirements**:
- Full JSP 936 documentation
- Independent ethics assurance mandatory
- Comprehensive V&V with independent review
- RAISO and Ethics Manager endorsement
- Submission to 2PUS (Second Permanent Under-Secretary) or Ministers

**Timeline**: 3-6 months for approval process

**Examples**:
- Autonomous weapons systems
- Strategic intelligence analysis AI
- High-stakes personnel decision AI

### Severe/Major (Score 10-19) → Defence-Level (JROC/IAC)

**Requirements**:
- Full JSP 936 documentation
- Independent ethics assurance recommended
- Comprehensive V&V
- RAISO and Ethics Manager endorsement
- Submission to Defence Joint Requirements Oversight Council (JROC) or Investment Approvals Committee (IAC)

**Timeline**: 2-4 months for approval process

**Examples**:
- Operational planning decision support
- Large-scale logistics optimization
- Critical infrastructure monitoring

### Moderate/Minor (Score 1-9) → TLB-Level (Delegated)

**Requirements**:
- JSP 936 documentation
- Standard V&V
- RAISO and Ethics Manager review
- Approval by Top-Level Budget (TLB) holder

**Timeline**: 1-2 months for approval process

**Examples**:
- Administrative process automation
- Non-critical data analysis
- Internal tools and productivity aids

---

## Integration with Other ArcKit Commands

### Before JSP 936 Assessment

```bash
# Establish project foundation
/arckit.principles       # Define architectural principles including ethical AI
/arckit.stakeholders     # Identify RAISO, Ethics Manager, AI operators
/arckit.requirements     # Document functional and ethical requirements

# Design system
/arckit.architecture     # Design human-AI interaction points
/arckit.data-model       # Document training data sources and flows
/arckit.diagram          # Visualize AI component integration
```

### During JSP 936 Assessment

```bash
# Create comprehensive AI assurance documentation
/arckit.jsp-936          # Generate JSP 936 assessment
```

### After JSP 936 Assessment

```bash
# Address findings and track compliance
/arckit.mod-secure       # AI-specific security assessment (complements JSP 936)
/arckit.backlog          # Track JSP 936 compliance tasks and gaps
/arckit.traceability     # Map ethical requirements to implementation
/arckit.service-assessment  # Prepare for service assessment (if public-facing)

# Ongoing
# Annual re-assessment with /arckit.jsp-936
# Continuous monitoring per Section 9 of JSP 936 documentation
```

---

## Common JSP 936 Patterns

### Pattern 1: Image Classification System

**AI Component**: Convolutional Neural Network (CNN)
**Risk Classification**: Major (Score 12: Likelihood 3 × Impact 4)
**Human-AI Teaming**: Human-on-loop
**Key Ethical Considerations**: Bias in training data, false positive/negative rates, adversarial robustness
**Approval**: Defence-Level (JROC/IAC)

**Critical Requirements**:
- Diverse training dataset across demographics and environments
- Bias testing for protected characteristics
- Adversarial robustness testing
- Explainable AI (CAM/Grad-CAM heatmaps)
- Human monitoring dashboard with confidence scores
- Override capability for operators

### Pattern 2: Decision Support System

**AI Component**: Ensemble model (Random Forest + XGBoost)
**Risk Classification**: Moderate (Score 6: Likelihood 2 × Impact 3)
**Human-AI Teaming**: Human-in-loop
**Key Ethical Considerations**: Recommendation bias, over-reliance, explainability
**Approval**: TLB-Level

**Critical Requirements**:
- Clear presentation of recommendations (not decisions)
- Feature importance explanations
- Alternative options presented
- User training on AI limitations
- Decision logging for audit

### Pattern 3: Autonomous Vehicle Navigation

**AI Component**: Deep Reinforcement Learning + Computer Vision
**Risk Classification**: Critical (Score 20: Likelihood 4 × Impact 5)
**Human-AI Teaming**: Human-on-loop with automatic safety override
**Key Ethical Considerations**: Safety-critical, adversarial scenarios, edge cases
**Approval**: 2PUS or Ministerial

**Critical Requirements**:
- Independent ethics assurance mandatory
- Extensive V&V in operational environment
- Safety envelope with automatic shutdown
- Comprehensive simulation testing
- Adversarial scenario testing
- Real-time monitoring with human takeover capability
- Regular safety audits

### Pattern 4: Large Language Model (LLM) for Document Analysis

**AI Component**: Fine-tuned transformer model (LLM)
**Risk Classification**: Moderate (Score 9: Likelihood 3 × Impact 3)
**Human-AI Teaming**: Human-on-loop
**Key Ethical Considerations**: Hallucinations, bias, data leakage, prompt injection
**Approval**: TLB-Level

**Critical Requirements**:
- No classified data in prompts unless approved LLM
- Hallucination detection and flagging
- Bias testing across topics and entities
- Prompt injection defenses
- Human review of AI-generated summaries
- Data handling procedures (no external LLM APIs for sensitive data)

---

## Checklist: JSP 936 Compliance

Use this checklist to verify JSP 936 compliance:

### Ethical Risk Classification
- [ ] All AI/ML components identified and catalogued
- [ ] Impact assessed (1-5) across 5 dimensions
- [ ] Likelihood assessed (1-5) across 5 factors
- [ ] Risk score calculated (Likelihood × Impact)
- [ ] Classification assigned (Critical/Severe/Major/Moderate/Minor)
- [ ] Unacceptable risk check performed
- [ ] Approval pathway determined

### Five Ethical Principles
- [ ] **Human-Centricity**: Stakeholder engagement, human control, rights assessment
- [ ] **Responsibility**: Accountability mapping, RAISO/Ethics Manager assigned, decision traceability
- [ ] **Understanding**: Explainability implemented, documentation complete, training delivered
- [ ] **Bias & Harm**: Bias testing completed, harms identified, mitigations implemented
- [ ] **Reliability**: Performance boundaries defined, robustness tested, security controls verified

### AI Lifecycle Documentation
- [ ] **Phase 1 - Planning**: Use case justified, alternatives considered
- [ ] **Phase 2 - Requirements**: FR/NFR/ETH/SAF/SEC documented, HAZOP completed
- [ ] **Phase 3 - Architecture**: System design, failure modes, security architecture
- [ ] **Phase 4 - Algorithm Design**: Algorithm selection justified, explainability approach defined
- [ ] **Phase 5 - Model Development**: Training data documented, model card created, bias analyzed
- [ ] **Phase 6 - V&V**: Test plan executed, V&V report completed, UAT passed
- [ ] **Phase 7 - Integration & Use**: Deployment complete, SOPs documented, training delivered
- [ ] **Phase 8 - QA**: Compliance matrix complete, ethical review passed, security audit passed

### Governance & Approval
- [ ] RAISO assigned and engaged
- [ ] Ethics Manager assigned and engaged
- [ ] Independent assurance (if Critical classification)
- [ ] Governance board established
- [ ] Approval submitted to correct authority
- [ ] Approval granted (or conditions met)

### Human-AI Teaming
- [ ] Teaming model selected (in-loop/on-loop/out-of-loop)
- [ ] Training programme delivered
- [ ] Human control interface designed
- [ ] Trust calibration addressed
- [ ] Override mechanisms provided

### Secure by Design
- [ ] AI threat landscape assessed (6 threat categories)
- [ ] AI-specific security controls implemented
- [ ] Adversarial testing completed
- [ ] Penetration testing completed
- [ ] Security monitoring operational

### Continuous Monitoring
- [ ] Real-time performance monitoring
- [ ] Bias monitoring operational
- [ ] Security monitoring operational
- [ ] Drift detection implemented
- [ ] Retraining triggers defined
- [ ] Annual re-assessment scheduled

---

## Frequently Asked Questions

### Q: When is JSP 936 assessment mandatory?

**A**: JSP 936 assessment is mandatory for **all AI/ML systems deployed in MOD projects**, regardless of risk classification. The depth and rigor of assessment scales with risk level:
- **Critical/Severe**: Full assessment with independent assurance
- **Major/Moderate**: Full assessment
- **Minor**: Streamlined assessment

### Q: How long does JSP 936 assessment take?

**A**: Timeline varies by risk classification:
- **Initial Assessment**: 2-4 weeks (with ArcKit automation)
- **Independent Assurance** (Critical): +4-8 weeks
- **Approval Process**: 1-6 months depending on pathway
- **Annual Re-assessment**: 1-2 weeks

### Q: Who must be involved in JSP 936 assessment?

**A**: Minimum roles required:
- **RAISO** (Responsible AI Senior Officer) - Overall governance
- **Ethics Manager** - Day-to-day oversight
- **Technical Lead** - AI system implementation
- **Operational Commander** - Operational use context
- **Information Assurance** - Security and accreditation
- **Independent Assurance** (Critical classification only)

### Q: Can I use commercial off-the-shelf (COTS) AI?

**A**: Yes, but you must still complete JSP 936 assessment including:
- Supplier assurance (Section 8 of documentation)
- Data provenance verification
- Model transparency requirements
- Bias testing on MOD use cases
- Security assessment
- Integration with MOD governance

### Q: How does JSP 936 relate to JSP 440 (Defence IA)?

**A**: JSP 936 complements JSP 440:
- **JSP 440**: Information assurance, general security controls
- **JSP 936**: AI-specific ethical and security considerations
- Both required for AI systems handling classified information
- Security architecture must address both frameworks

### Q: What happens if risk classification increases after deployment?

**A**: Immediate actions required:
- Pause system use (if risk becomes unacceptable)
- Inform RAISO and Ethics Manager
- Conduct updated JSP 936 assessment
- Submit to new approval authority (higher level)
- Implement additional controls
- Only resume after re-approval

### Q: Is continuous monitoring mandatory?

**A**: Yes, for all risk classifications:
- **Critical/Severe/Major**: Real-time monitoring mandatory
- **Moderate/Minor**: At minimum, periodic (e.g., weekly) checks
- Drift detection required for all
- Annual re-assessment mandatory for all

### Q: Can AI systems be deployed without human control?

**A**: Only in **very limited circumstances**:
- Risk classification must be **Minor** (<5 score)
- Operational impact must be negligible
- Safety envelope with automatic shutdown required
- Annual review of human-out-of-loop justification
- **Never permitted** for weapon systems or critical decisions

---

## Example JSP 936 Scenarios

### Scenario 1: Satellite Imagery Analysis for Infrastructure Monitoring

**Context**: AI system analyzes satellite imagery to detect infrastructure changes in areas of interest.

**AI Components**:
- Computer vision model (CNN) for object detection
- Change detection algorithm comparing historical imagery

**Risk Assessment**:
- **Likelihood**: 3 (Possible - technology mature but environmental variability)
- **Impact**: 4 (Major - missed changes could affect operational planning)
- **Risk Score**: 12 (Major classification)
- **Approval**: Defence-Level (JROC/IAC)

**Key JSP 936 Considerations**:
- **Human-Centricity**: Analyst reviews all detected changes before action
- **Responsibility**: Clear chain from AI detection → Analyst review → Commander decision
- **Understanding**: Heatmaps show model attention, confidence scores displayed
- **Bias & Harm**: Tested across different geographies, weather conditions, image quality
- **Reliability**: Robustness tested with degraded imagery, adversarial patches

**Human-AI Teaming**: Human-on-loop (analyst monitors, flags high-confidence changes for urgent review)

### Scenario 2: Predictive Maintenance for Vehicle Fleet

**Context**: ML model predicts vehicle component failures based on sensor data and maintenance history.

**AI Components**:
- Supervised learning model (Random Forest) for failure prediction
- Anomaly detection for unusual sensor readings

**Risk Assessment**:
- **Likelihood**: 2 (Unlikely - well-established ML technique)
- **Impact**: 3 (Moderate - vehicle downtime affects operations but not critical)
- **Risk Score**: 6 (Moderate classification)
- **Approval**: TLB-Level

**Key JSP 936 Considerations**:
- **Human-Centricity**: Maintenance recommendations reviewed by qualified technician
- **Responsibility**: Maintenance supervisor approves work orders based on AI predictions
- **Understanding**: Feature importance shows which sensor readings drove prediction
- **Bias & Harm**: No bias concerns (vehicles, not people), harm limited to missed failures
- **Reliability**: Validated on historical data, false positive/negative rates within tolerance

**Human-AI Teaming**: Human-in-loop (technician reviews predictions before maintenance scheduling)

### Scenario 3: Autonomous Drone for Reconnaissance

**Context**: Autonomous drone navigates designated area, identifies points of interest, returns to base.

**AI Components**:
- Deep reinforcement learning for navigation and obstacle avoidance
- Computer vision for terrain mapping and object recognition
- Decision logic for "interesting" vs "not interesting" classifications

**Risk Assessment**:
- **Likelihood**: 4 (Likely - complex environment, adversarial potential)
- **Impact**: 5 (Catastrophic - drone crash could cause casualties or compromise mission)
- **Risk Score**: 20 (Critical classification)
- **Approval**: 2PUS or Ministerial

**Key JSP 936 Considerations**:
- **Human-Centricity**: Operator monitors flight, can take control or abort mission anytime
- **Responsibility**: Clear command chain, meaningful human control via abort/override
- **Understanding**: Operator dashboard shows flight plan, detected obstacles, AI confidence
- **Bias & Harm**: Extensive testing in diverse environments, adversarial scenario testing
- **Reliability**: Redundant systems, automatic return-to-base on failures, geofencing

**Human-AI Teaming**: Human-on-loop with automatic safety overrides (operator monitors, autonomous within safety envelope)

**Independent Ethics Assurance**: Required due to Critical classification

---

## Additional Resources

### UK MOD References
- **JSP 936**: Dependable Artificial Intelligence (AI) in Defence (November 2024)
- **JSP 440**: Defence Information Assurance (IA) Policy
- **DSA01-DES**: Defence Systems Approach to Safety
- **DefStan 00-056**: Safety Management Requirements for Defence Systems

### UK Government AI Guidance
- **Government AI Assurance Roadmap** (Cabinet Office)
- **Data Ethics Framework** (CDDO)
- **Algorithmic Transparency Standard** (CDDO)
- **AI Safety Institute** guidance (DSIT)

### International Standards
- **ISO/IEC 42001**: AI Management System
- **ISO/IEC 23894**: AI Risk Management
- **IEEE 7000 series**: Ethical AI standards
- **NATO AI Strategy**: Allied defence AI policy

### Academic Resources
- UK Defence Academy AI research
- Alan Turing Institute - AI ethics and assurance
- Oxford Internet Institute - AI governance
- Cambridge Centre for the Study of Existential Risk

---

## Getting Help

### ArcKit Support
- **Documentation**: [https://github.com/tractorjuice/arc-kit](https://github.com/tractorjuice/arc-kit)
- **Issues**: [GitHub Issues](https://github.com/tractorjuice/arc-kit/issues)

### MOD AI Assurance Support
- **RAISO Network**: Contact your TLB's Responsible AI Senior Officer
- **Defence AI Centre**: MOD AI Centre of Excellence
- **Defence Digital**: AI and data governance support

### Related ArcKit Guides
- [MOD Secure by Design Guide](mod-secure.md) - Complements JSP 936 with detailed security controls
- [Risk Management Guide](risk-management.md) - HM Treasury Orange Book risk assessment
- [Service Assessment Guide](service-assessment.md) - GDS service standard for public-facing services
- [Requirements Guide](requirements.md) - Documenting ethical requirements (ETH requirements)
- [Architecture Guide](architecture.md) - Designing human-AI interaction points

---

*This guide reflects JSP 936 as published November 2024. Always refer to the latest MOD policy for authoritative guidance.*
